# ğŸ“„ AI-Powered RAG-Based PDF Q&A System with Local LLM ğŸš€

## ğŸ›  Problem Statement
In today's fast-paced world, retrieving relevant information from large PDF documents can be tedious and time-consuming. Traditional keyword-based search often fails to provide context-aware answers. A more intelligent approach is needed to extract meaningful insights from documents effectively.

## â“ Why RAG (Retrieval-Augmented Generation)?
RAG combines **retrieval-based** and **generation-based** approaches to enhance the accuracy of responses. Instead of relying solely on a pre-trained model's knowledge, RAG fetches relevant document sections and generates context-aware answers. This improves:

âœ… **Precision** â€“ Only relevant text is used for responses.  
âœ… **Scalability** â€“ Works well with large document collections.  
âœ… **Flexibility** â€“ Can be integrated with various LLMs for local/private deployment.  

---

# ğŸ”§ Tools Used
- **Streamlit** â€“ For building an interactive web-based UI  
- **LangChain** â€“ For document loading, text splitting, retrieval, and LLM integration  
- **PDFPlumber** â€“ To extract text from PDF documents  
- **Ollama** â€“ For running local LLMs  
- **InMemoryVectorStore** â€“ To store and retrieve document embeddings  
- **DeepSeek / Any Local LLM** â€“ As the language model for answering queries  

---

# ğŸ”„ Query-to-Response Workflow

## ğŸ“‚ 1. Upload PDF
Users can upload a PDF document via the **Streamlit** interface. The uploaded file is saved to a designated directory for processing.

## ğŸ“‘ 2. Load & Parse PDF
Using **PDFPlumber**, the system extracts text content from the document and converts it into a structured format.

## âœ‚ 3. Text Splitting
The extracted text is split into smaller, manageable chunks using **RecursiveCharacterTextSplitter**. This ensures that each chunk is of optimal length for embedding and retrieval.

## ğŸ§  4. Embedding & Vector Storage
Each text chunk is converted into a numerical representation using **OllamaEmbeddings** (or any local embedding model). The embeddings are stored in an **InMemoryVectorStore** for efficient retrieval.

## ğŸ” 5. Query Processing & Retrieval
When a user inputs a query, the system performs a similarity search in the vector database to retrieve the most relevant text chunks.

## ğŸ¤– 6. Response Generation
The retrieved context is passed to an **Ollama LLM** (or any local model). A structured prompt template ensures the model generates concise, context-aware responses.

## ğŸ’¬ 7. Display Answer
The generated response is displayed in a **Streamlit chat interface**, allowing users to interact dynamically with the system.

---

# ğŸ–¥ Using a Local LLM
This system is designed to work with **local LLMs** using [Ollama](https://ollama.com/), ensuring privacy and offline accessibility.  
### Steps to Run with a Local LLM:
1ï¸âƒ£ Install **Ollama** and download a compatible model (e.g., `deepseek-r1:7b`).  
2ï¸âƒ£ Configure the LLM within the LangChain pipeline.  
3ï¸âƒ£ Run the Streamlit app and start querying your PDFs! ğŸ‰  

---

# ğŸ“Œ Important Notes
- This system can be extended to use other **LLMs** like Llama, Mistral, or GPT-based models.
- Future enhancements may include **multi-document support**, **database-backed storage**, and **improved UI**.
- Ensure that your local model is optimized for **inference speed** to maintain a smooth user experience.

---

ğŸš€ **Ready to revolutionize how you search PDFs? Try it now!** ğŸ¯

